{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgwZtzCyvTYf"
   },
   "source": [
    "# Семинар по обработке текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbHlWheTvTYi"
   },
   "source": [
    "### Примеры задач автоматической обработки текстов:\n",
    "\n",
    "- классификация текстов\n",
    "\n",
    "    - анализ тональности (например, позитивный/негативный отзыв)\n",
    "    - фильтрация спама\n",
    "    - по теме или жанру\n",
    "\n",
    "- машинный перевод\n",
    "\n",
    "- распознавание и синтез речи\n",
    "\n",
    "- извлечение информации\n",
    "\n",
    "    - именованные сущности (например, извлечение имен, локаций, названий организаций)\n",
    "    - извлечение фактов и событий\n",
    "\n",
    "- кластеризация текстов\n",
    "\n",
    "- оптическое распознавание символов\n",
    "\n",
    "- проверка правописания\n",
    "\n",
    "- вопросно-ответные системы, информационный поиск\n",
    "\n",
    "- суммаризация текстов\n",
    "\n",
    "- генерация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krXjGbjhvTYj"
   },
   "source": [
    "### Классические методы для работы с текстами:\n",
    "\n",
    "- токенизация\n",
    "\n",
    "- лемматизация / стемминг\n",
    "\n",
    "- удаление стоп-слов и пунктуации\n",
    "\n",
    "- векторное представление текстов (bag of words и TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDTZ7GNIvTYj"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- Jurafsky, Martin: Speech and Language Processing (2nd or 3rd Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTYWd8k5vTYk"
   },
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизировать — значит, поделить текст на слова, или *токены*.\n",
    "\n",
    "Самый наивный способ токенизировать текст — разделить с помощью `split`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wYTyUXsvTYk",
    "outputId": "55a6eabb-3b53-4502-d9b3-437f1847434a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nikita\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hCRd91OvTYl",
    "outputId": "9f180778-098e-4a9d-a3cc-aa3cdf905f6d"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BnwtbxVSvTYl"
   },
   "outputs": [],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2GQ7QsjvTYm",
    "outputId": "784207cc-6ba2-4e96-8066-d733c7786685"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c помощью split()\n",
    "example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA0DyFIDvTYn",
    "outputId": "df8fdb70-4d7a-476b-9496-78562e7e5912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c помощью токенизатора\n",
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ7uHEJUvTYo"
   },
   "source": [
    "В nltk вообще есть довольно много токенизаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fnNryNkvTYo",
    "outputId": "0aae1ea4-65f5-465c-eda4-199c3641b232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'SyllableTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordTokenizer']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cs6tdtHvTYp"
   },
   "source": [
    "Можно получить индексы начала и конца каждого токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Но не каждый хочет что-то исправлять:('"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v3kOepXRvTYp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeMsc1yVvTYp"
   },
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5diW1OKkvTYq",
    "outputId": "0d12988f-05ec-433c-f25d-357a1adfc61a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4Jte9_YvTYq"
   },
   "source": [
    "Для некоторых задач это может быть полезно.\n",
    "\n",
    "А некоторые предназначены вообще не для текста на естественном языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vC4pjzzWvTYr",
    "outputId": "ee796848-f30a-45aa-b2fb-11d0d0c87a31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(a (b c))', 'd', 'e', '(f)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTLQY6ndwP9u"
   },
   "source": [
    "Есть токенизатор, который может быть полезен для работы с твитами или сообщениями из соц. сетей.\n",
    "Он сохранит смайлики, хештеги и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xioD_3G2vTYs",
    "outputId": "155e9e32-9d62-4223-d575-df678aca92c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'picture', 'on', 'the', 'wall']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "example = 'This is a picture on the wall'\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "tw.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxWB7BzKvTYs"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- http://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/\n",
    "- https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc1OYy0tvTYt"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* — это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2F7i9CEvTYt",
    "outputId": "e8d5b428-583d-46a3-ac67-7f6ed7763dfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8NdjFCYvTYt",
    "outputId": "bab6dba5-a1d6-480a-8a79-998de1c6c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2DehgAP3vTYu",
    "outputId": "25f5ddde-9521-4a76-803e-126ad14b1a3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gLzygOQ6vTYv"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SEdeiiPvTYv"
   },
   "source": [
    "## Лемматизация\n",
    "\n",
    "[**Лемматизация**](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве.\n",
    "\n",
    "\n",
    "Например, токены «пью», «пил», «пьет» перейдут в «пить». Почему это хорошо?\n",
    "* Во-первых, мы хотим рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: `mystem` и `pymorphy`.\n",
    "\n",
    "### [Mystem](https://tech.yandex.ru/mystem/)\n",
    "Как с ним работать:\n",
    "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
    "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM5dc5MPvTYw",
    "outputId": "fa540fd2-7a6a-476f-ab32-b0fd37e86461",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from pymystem3) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from requests->pymystem3) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nikita\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.0.4)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "60Q4xp6yvTYw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to C:\\Users\\nikita/.local/bin\\mystem.exe from http://download.cdn.yandex.net/mystem/mystem-3.1-win-64bit.zip\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "import pymystem3\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al294srlvTYx"
   },
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin — путь к `mystem`, если их несколько\n",
    "* grammar_info — нужна ли грамматическая информация или только леммы (по умолчанию нужна)\n",
    "* disambiguation — нужно ли снятие [омонимии](https://ru.wikipedia.org/wiki/%D0%9E%D0%BC%D0%BE%D0%BD%D0%B8%D0%BC%D1%8B) - дизамбигуация (по умолчанию нужна)\n",
    "* entire_input — нужно ли сохранять в выводе все (пробелы, например), или можно выкинуть (по умолчанию оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'Картину сняли со стены'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Oc4plDGvTYx",
    "outputId": "dd71e6d0-a98b-4a5c-fa3c-30370a493425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['картина', ' ', 'снимать', ' ', 'со', ' ', 'стена', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mX1O12xvTYy"
   },
   "source": [
    "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7-NcH9ivTYy",
    "outputId": "52fc5c7e-408c-4838-c022-84c9c340349b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=34a765d9a5ba1694c43687337c3f750be78a26eb8f66f85f9c41b0c7d21b525c\n",
      "  Stored in directory: c:\\users\\nikita\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Collecting pymorphy2-dicts\n",
      "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
      "Installing collected packages: pymorphy2-dicts\n",
      "Successfully installed pymorphy2-dicts-2.4.393442.3710985\n",
      "Requirement already satisfied: DAWG-Python in c:\\users\\nikita\\anaconda3\\lib\\site-packages (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install pymorphy2-dicts\n",
    "!pip install DAWG-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jnL4uv6JvTYz"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIe3ssarvTYz"
   },
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение — он его просто не лемматизирует, т.к. не понимает.\n",
    "\n",
    "Метод MorphAnalyzer.parse() принимает слово  и возвращает все возможные его разборы.\n",
    "\n",
    "У каждого разбора есть тег.\n",
    "Тег - это набор граммем, характеризующих данное слово. Например, тег 'VERB,perf,intr plur,past,indc' означает, что слово - глагол (VERB) совершенного вида (perf), непереходный (intr), множественного числа (plur), прошедшего времени (past), изъявительного наклонения (indc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ztv5u7gvTY0",
    "outputId": "b70bae89-562b-4ce3-ba4c-e10a10469fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хочет', tag=OpencorporaTag('VERB,impf,tran sing,3per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'хочет', 3136, 5),))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy2_analyzer.parse('хочет')\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9NBnUjhfvTY2",
    "outputId": "45f3f0ca-770e-4436-a0d0-9006562c4d86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY0CeomBvTY3"
   },
   "source": [
    "### mystem vs. pymorphy\n",
    "\n",
    "1) *Надеемся, что вы пользуетесь линуксом или маком* — mystem работает невероятно медленно под windows на больших текстах\n",
    "\n",
    "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZHNWM8MvTY3",
    "outputId": "516e91d4-e73b-41bb-f01b-26f6643ff7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
      "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
     ]
    }
   ],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
    "\n",
    "# корректно определелил части речи\n",
    "# NUM - числительное\n",
    "# S — существительное\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4ifiT5avTY4"
   },
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3tkqE7jvTY4"
   },
   "source": [
    "В отличие от лемматизации, при применении стемминга у всех слов отбрасываются аффиксы (окончания и суффиксы), что необязательно приводит слова к формам, существующим в рассматриваемом языке.\n",
    "\n",
    "[**Snowball**](http://snowball.tartarus.org/) – фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке – списки окончаний для разных чистей речи, разных склонений и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tUI4LkoUvTY4"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDhsgmNsvTY5",
    "outputId": "9b29e2a4-1a9e-4e10-f247-3abdb81b14d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "но не кажд хочет что-т исправля : (\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('russian')\n",
    "stemmed_example = [stemmer.stem(w) for w in tokenized_example]\n",
    "print(' '.join(stemmed_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для английского получится что-то такое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYdj8R20vTY5",
    "outputId": "1bd95bdf-b5f2-42c2-a479-acdf03c58809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\n",
      "\"Whenever you feel like criticizing any one,\" he told me, \"just remember that all the people in this world haven't had the advantages that you've had.\"\n",
      "==========\n",
      "['In', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'I', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', 'Whenever', 'you', 'feel', 'like', 'criticizing', 'any', 'one', 'he', 'told', 'me', 'just', 'remember', 'that', 'all', 'the', 'people', 'in', 'this', 'world', 'have', 'had', 'the', 'advantages', 'that', 'you', 'had']\n"
     ]
    }
   ],
   "source": [
    "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"\"\n",
    "print(text)\n",
    "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "print('==========')\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "386kIQJYvTY5",
    "outputId": "3e3efc61-d8ec-4af6-d907-598ab3acb7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulner year my father gave me some advic that i been turn over in my mind ever sinc whenev you feel like critic ani one he told me just rememb that all the peopl in this world have had the advantag that you had\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print(' '.join(text_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS3FTOIXvTY6"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stemming\n",
    "- https://en.wikipedia.org/wiki/Lemmatisation\n",
    "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks-scoLGvTY6"
   },
   "source": [
    "## Bag-of-words и TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5dNFUPHvTY6"
   },
   "source": [
    "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Нужна выборка!\n",
    "\n",
    "\n",
    "### Bag-of-words\n",
    "\n",
    "Самый очевидный способ формирования признакового описания текстов — векторизация. Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
    "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
    "\n",
    "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "x2eF8nZLvTY7"
   },
   "outputs": [],
   "source": [
    "texts = ['I like my cat.', 'My cat is the most perfect cat.', 'is this cat or is this bread?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat.',\n",
       " 'My cat is the most perfect cat.',\n",
       " 'is this cat or is this bread?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjG4gU5JvTY7",
    "outputId": "0d8650c5-7419-4689-ca67-c7729734991f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat',\n",
       " 'My cat is the most perfect cat',\n",
       " 'is this cat or is this bread']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NVomQglIvTY8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwAKWt_4vTY9",
    "outputId": "1c8f8dad-89a9-4402-adae-916f76ae1631"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikita\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ye63VKazvTY-",
    "outputId": "4dede0ed-ac68-4bad-ef3e-bb5be72f4dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvYOzpALvTY-",
    "outputId": "0b3192e7-2546-4040-cac8-55b208b30071"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 2, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 2, 0, 0, 0, 1, 0, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "z40jdf5-vTY-",
    "outputId": "80b68939-a2b0-4ea1-909e-409e9b1b6310"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bread  cat  is  like  most  my  or  perfect  the  this\n",
       "0      0    1   0     1     0   1   0        0    0     0\n",
       "1      0    2   1     0     1   1   0        1    1     0\n",
       "2      1    1   2     0     0   0   1        0    0     2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tZsL30hvTY-"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. Для того, чтобы разделять эти такие слова, можно использовать статистическую меру TF-IDF, характеризующую важность слова для конкретного текста. Для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
    "$$\n",
    "где $C(t | d)$ - число вхождений слова $t$ в текст $d$.\n",
    "\n",
    "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right)\n",
    "$$\n",
    "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
    "\n",
    "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "Интерпретировать формулу выше несложно: действительно, чем чаще данное слово встречается в данном тексте и чем реже в остальных, тем важнее оно для этого текста.\n",
    "\n",
    "Отметим, что в качестве TF и IDF можно использовать другие [определения](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat',\n",
       " 'My cat is the most perfect cat',\n",
       " 'is this cat or is this bread']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "F0YcEWeqvTY_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpCVQJG8vTY_",
    "outputId": "f8290c90-e00b-4cef-cff8-538702ff4ba7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikita\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saJG0dWyvTZA",
    "outputId": "de21ac1d-4ae3-4cb3-86ed-d1a9dac5fe9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t43byRVIvTZA",
    "outputId": "ad00d857-ded7-4f95-f58f-f41765ece04b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
       "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
       "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
       "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
       "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "YGpSpl4KvTZA",
    "outputId": "266e8fdd-a1ae-4c1c-a4b8-2a950abd0e03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501310</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.200671</td>\n",
       "      <td>0.516802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.679533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bread       cat        is      like      most        my        or  \\\n",
       "0  0.000000  0.425441  0.000000  0.720333  0.000000  0.547832  0.000000   \n",
       "1  0.000000  0.501310  0.322764  0.000000  0.424396  0.322764  0.000000   \n",
       "2  0.339766  0.200671  0.516802  0.000000  0.000000  0.000000  0.339766   \n",
       "\n",
       "    perfect       the      this  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.424396  0.424396  0.000000  \n",
       "2  0.000000  0.000000  0.679533  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iwcgx05UvTZA"
   },
   "source": [
    "**Вопросик:** Что изменилось по сравнению с использованием метода `CountVectorizer`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVDDSrJNvTZB"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrP93rWwvTZB"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "mMoCnsi5vTZB"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUAoWQ1-vTZC"
   },
   "source": [
    "Что такое n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KeFXNVbPvTZC"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIf_da6RvTZD",
    "outputId": "9e811e97-bc9b-4bc4-a980-b164d1a5a575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Если б мне платили каждый раз'.split()\n",
    "list(ngrams(sent, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhaYTVyuvTZD",
    "outputId": "dbd0ae5c-d5eb-4e1f-e501-7e54476014b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fpat_ZGvTZD",
    "outputId": "a49249bd-6199-41ff-85cd-fe108b2a684a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wK2QdeycvTZE",
    "outputId": "c8dc6ac6-1ee8-4cc1-8526-47cd8dbfaa40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 5)) # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1OpE-oFvTZE"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
    "\n",
    "Классификацию по тональности используют, например, в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNmExWmlvTZF",
    "outputId": "148f293b-12b9-479f-e801-32a62786b5f2"
   },
   "outputs": [],
   "source": [
    "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
    "#!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "#!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "km4cEWTavTZF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "LLrigaDsvTZG"
   },
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = 'positive'\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = 'negative'\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "QHjtzEsMvTZG",
    "outputId": "b4404299-e371-48da-f369-40f37e7b46eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  @first_timee хоть я и школота, но поверь, у на...  positive\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIvPNAM5vTZH",
    "outputId": "5363dec1-d738-4d40-910d-c7ffba3e86de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226834, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "5iQv8gWdvTZH"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50571     690064/1 давай ходить со мной? будешь  отпраши...\n",
       " 99715     @latavika даже на еду не так много времени ухо...\n",
       " 18306     в #Shazam Угар в исполнении DJ M.E.G. Feat. Se...\n",
       " 96465     @YesMolotowa кажется в трудовом кодексе даже н...\n",
       " 56711     С любимой встречаем первый нормальный снег:* h...\n",
       "                                 ...                        \n",
       " 25324     @Ure_Amigo лузер :D лол, хотя в прошлые выходн...\n",
       " 65689     Ууух, таки я приполз с баскетбола. Как и предп...\n",
       " 103957    @ddenchik @aleksandrra_k @Evgeshuk Саня на фут...\n",
       " 49795     Хреново,когда тебе предлагают поехать отдохнут...\n",
       " 63441     RT @dearlolitaa: какой то это самый херовый но...\n",
       " Name: text, Length: 170125, dtype: object,\n",
       " 50571     positive\n",
       " 99715     negative\n",
       " 18306     positive\n",
       " 96465     positive\n",
       " 56711     positive\n",
       "             ...   \n",
       " 25324     positive\n",
       " 65689     positive\n",
       " 103957    negative\n",
       " 49795     negative\n",
       " 63441     negative\n",
       " Name: label, Length: 170125, dtype: object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwtsmgibvTZI"
   },
   "source": [
    "Как уже обсуждали, самый простой способ извлечь признаки из текстовых данных — векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает простую вещь:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "8GdQ8RmyvTZI"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train) # bow — bag of words (мешок слов)\n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<170125x243421 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1847716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E64CpRsjvTZJ"
   },
   "source": [
    "ngram_range отвечает за то, какие n-граммы мы используем в качестве фичей:<br/>\n",
    "ngram_range=(1, 1) — униграммы<br/>\n",
    "ngram_range=(3, 3) — триграммы<br/>\n",
    "ngram_range=(1, 3) — униграммы, биграммы и триграммы.\n",
    "\n",
    "В `vec.vocabulary_` лежит словарь: мэппинг слов к их индексам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ytgHKnjvTZJ",
    "outputId": "623848b0-b7d4-42b2-a985-59c14f92a8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('690064', 4548),\n",
       " ('давай', 122764),\n",
       " ('ходить', 233469),\n",
       " ('со', 212154),\n",
       " ('мной', 159037),\n",
       " ('будешь', 108431),\n",
       " ('отпрашиваться', 176489),\n",
       " ('на', 161516),\n",
       " ('два', 123438),\n",
       " ('часа', 235947),\n",
       " ('latavika', 50076),\n",
       " ('даже', 122931),\n",
       " ('еду', 129960),\n",
       " ('не', 165135),\n",
       " ('так', 219275),\n",
       " ('много', 158966),\n",
       " ('времени', 115123),\n",
       " ('уходит', 229034),\n",
       " ('shazam', 77420),\n",
       " ('угар', 225516),\n",
       " ('исполнении', 141229),\n",
       " ('dj', 25871),\n",
       " ('feat', 31072),\n",
       " ('serebro', 76854),\n",
       " ('ноавится', 168349),\n",
       " ('песня', 181317),\n",
       " ('song', 79360),\n",
       " ('moscow', 59064),\n",
       " ('music', 59987),\n",
       " ('россия', 204545),\n",
       " ('http', 37879),\n",
       " ('co', 21449),\n",
       " ('240o5b6ttr', 2032),\n",
       " ('yesmolotowa', 94317),\n",
       " ('кажется', 142272),\n",
       " ('трудовом', 223720),\n",
       " ('кодексе', 145912),\n",
       " ('нет', 167372),\n",
       " ('такой', 219340),\n",
       " ('статьи', 216006),\n",
       " ('любимой', 154233),\n",
       " ('встречаем', 115676),\n",
       " ('первый', 179692),\n",
       " ('нормальный', 168847),\n",
       " ('снег', 211804),\n",
       " ('sius3mjwpc', 78252),\n",
       " ('stammru', 80087),\n",
       " ('whenever', 91384),\n",
       " ('это', 241327),\n",
       " ('просто', 196975),\n",
       " ('конфигуратор', 147364),\n",
       " ('для', 126069),\n",
       " ('cron', 22089),\n",
       " ('хочется', 234087),\n",
       " ('внутри', 113373),\n",
       " ('приложения', 193862),\n",
       " ('таймить', 219238),\n",
       " ('без', 104177),\n",
       " ('redis', 72791),\n",
       " ('resque', 73033),\n",
       " ('juliamayko', 43185),\n",
       " ('donna_132', 26504),\n",
       " ('irina_sharova1', 40505),\n",
       " ('прямо', 197781),\n",
       " ('каг', 142188),\n",
       " ('мультиге', 160646),\n",
       " ('rt', 74379),\n",
       " ('othertribunus', 66927),\n",
       " ('провокация', 195273),\n",
       " ('кгб', 144189),\n",
       " ('меня', 157383),\n",
       " ('разлился', 200137),\n",
       " ('чай', 235842),\n",
       " ('все', 115250),\n",
       " ('планы', 182766),\n",
       " ('сегодня', 207718),\n",
       " ('обломались', 170338),\n",
       " ('фазой', 229530),\n",
       " ('будем', 108421),\n",
       " ('сидеть', 208841),\n",
       " ('моей', 159260),\n",
       " ('племяшкой', 183002),\n",
       " ('dmaijac', 26084),\n",
       " ('ну', 169188),\n",
       " ('вот', 114768),\n",
       " ('столько', 216509),\n",
       " ('прожил', 195851),\n",
       " ('про', 194855),\n",
       " ('знал', 138149),\n",
       " ('sashal1999', 76011),\n",
       " ('kisa_tanya2000', 46574),\n",
       " ('рифма', 203774),\n",
       " ('хуифма', 234564),\n",
       " ('моя', 160256),\n",
       " ('любимая', 154213),\n",
       " ('polliqo', 69657),\n",
       " ('да', 122699),\n",
       " ('нах', 164727),\n",
       " ('повод', 183812),\n",
       " ('если', 130401),\n",
       " ('такая', 219287),\n",
       " ('красотка', 148947),\n",
       " ('форме', 231079),\n",
       " ('арестует', 100988),\n",
       " ('ахахахахахах', 101944),\n",
       " ('блять', 106397),\n",
       " ('истерика', 141467),\n",
       " ('чувак', 237404),\n",
       " ('ты', 224405),\n",
       " ('капец', 143066),\n",
       " ('красавчик', 148781),\n",
       " ('лучший', 154022),\n",
       " ('знаю', 138227),\n",
       " ('что', 237333),\n",
       " ('еще', 130560),\n",
       " ('сказать', 209485),\n",
       " ('хотя', 234008),\n",
       " ('уже', 226263),\n",
       " ('сказано', 209477),\n",
       " ('бобра', 106484),\n",
       " ('тебе', 220428),\n",
       " ('снеег', 211863),\n",
       " ('___', 6547),\n",
       " ('улице', 226797),\n",
       " ('крууто', 149826),\n",
       " ('uyfdunho2v', 86987),\n",
       " ('anutochkasavina', 13730),\n",
       " ('лагерь', 151171),\n",
       " ('круговорот', 149654),\n",
       " ('выступил', 117876),\n",
       " ('визиткой', 112195),\n",
       " ('фестивале', 230195),\n",
       " ('пед', 179374),\n",
       " ('команд', 146473),\n",
       " ('зажгли', 133757),\n",
       " ('kuqcwsuhss', 49126),\n",
       " ('интернетоооом', 140628),\n",
       " ('вообще', 114255),\n",
       " ('грузит', 121975),\n",
       " ('elena13092011', 28567),\n",
       " ('видимо', 112085),\n",
       " ('удастся', 225863),\n",
       " ('скинуть', 209780),\n",
       " ('ильзира', 139902),\n",
       " ('тебя', 220436),\n",
       " ('хочет', 234084),\n",
       " ('отвечаешь', 175235),\n",
       " ('yrnkjfoy3b', 95000),\n",
       " ('наверно', 161766),\n",
       " ('99', 6125),\n",
       " ('людей', 154439),\n",
       " ('сейчас', 207831),\n",
       " ('хорошое', 233909),\n",
       " ('настроение', 164291),\n",
       " ('сердце', 208338),\n",
       " ('потому', 190545),\n",
       " ('статусе', 215989),\n",
       " ('почек', 191092),\n",
       " ('vonsoltan', 90035),\n",
       " ('ifapperpro', 38926),\n",
       " ('уж', 226186),\n",
       " ('думал', 128706),\n",
       " ('нашел', 165036),\n",
       " ('того', 221850),\n",
       " ('самого', 205872),\n",
       " ('там', 219500),\n",
       " ('бмв', 106442),\n",
       " ('фотографиях', 231276),\n",
       " ('харя', 232523),\n",
       " ('как', 142435),\n",
       " ('коли', 146211),\n",
       " ('чешского', 236786),\n",
       " ('но', 168346),\n",
       " ('днепр', 126130),\n",
       " ('nykeddi', 64602),\n",
       " ('мне', 158922),\n",
       " ('лишь', 153239),\n",
       " ('через', 236391),\n",
       " ('рамку', 200876),\n",
       " ('просили', 196749),\n",
       " ('пройти', 196077),\n",
       " ('обычно', 171558),\n",
       " ('когда', 145878),\n",
       " ('брату', 107842),\n",
       " ('приходишь', 194638),\n",
       " ('hmtdl9atsg', 37350),\n",
       " ('gadzhi_fcb', 33250),\n",
       " ('madonna333999', 54387),\n",
       " ('b_aisa', 16065),\n",
       " ('што', 239372),\n",
       " ('то', 221785),\n",
       " ('другое', 128429),\n",
       " ('miss_death_nigh', 58144),\n",
       " ('хмм', 233372),\n",
       " ('меломэн', 157247),\n",
       " ('же', 130967),\n",
       " ('под', 184243),\n",
       " ('рок', 204284),\n",
       " ('времена', 115119),\n",
       " ('плюнь', 183257),\n",
       " ('моду', 159242),\n",
       " ('погоду', 184140),\n",
       " ('слушай', 211116),\n",
       " ('цени', 235407),\n",
       " ('свободу', 207142),\n",
       " ('почему', 191096),\n",
       " ('мое', 159256),\n",
       " ('приложение', 193857),\n",
       " ('вк', 112714),\n",
       " ('заходит', 136882),\n",
       " ('koonstantinova', 47379),\n",
       " ('зая', 137194),\n",
       " ('жалко', 130699),\n",
       " ('люблю', 154295),\n",
       " ('biankanumber1', 17603),\n",
       " ('singerika', 78144),\n",
       " ('трудновато', 223694),\n",
       " ('такому', 219345),\n",
       " ('аду', 98389),\n",
       " ('будет', 108427),\n",
       " ('зато', 136569),\n",
       " ('раю', 202174),\n",
       " ('вип', 112472),\n",
       " ('полюбо', 187874),\n",
       " ('сонёлю', 213259),\n",
       " ('досталось', 127829),\n",
       " ('партий', 178859),\n",
       " ('ни', 167740),\n",
       " ('одной', 172238),\n",
       " ('даж', 122927),\n",
       " ('сонджона', 213181),\n",
       " ('есть', 130452),\n",
       " ('партия', 178863),\n",
       " ('понимаешь', 188398),\n",
       " ('был', 109156),\n",
       " ('человек', 236223),\n",
       " ('вдруг', 110562),\n",
       " ('за', 132223),\n",
       " ('один', 172094),\n",
       " ('миг', 157925),\n",
       " ('его', 129818),\n",
       " ('стало', 215636),\n",
       " ('sviridenko_ok', 81344),\n",
       " ('зачем', 136994),\n",
       " ('думать', 128714),\n",
       " ('можно', 159300),\n",
       " ('сразу', 215201),\n",
       " ('подписать', 185233),\n",
       " ('pyrotogoxyb', 70944),\n",
       " ('макбуков', 155158),\n",
       " ('видать', 112002),\n",
       " ('странно', 216737),\n",
       " ('nika9235', 63106),\n",
       " ('direction4everf', 25743),\n",
       " ('слишком', 210799),\n",
       " ('нереально', 167122),\n",
       " ('ненормально', 166505),\n",
       " ('каждого', 142251),\n",
       " ('скелеты', 209715),\n",
       " ('шкафу', 238794),\n",
       " ('которые', 148416),\n",
       " ('будут', 108484),\n",
       " ('преследовать', 192667),\n",
       " ('нас', 163978),\n",
       " ('всю', 115773),\n",
       " ('жизнь', 131528),\n",
       " ('заебался', 133662),\n",
       " ('ездить', 130071),\n",
       " ('домой', 127303),\n",
       " ('пять', 198958),\n",
       " ('часов', 235960),\n",
       " ('вечера', 111479),\n",
       " ('кто', 150031),\n",
       " ('ооо', 173352),\n",
       " ('инга', 140167),\n",
       " ('ахахаха', 101904),\n",
       " ('дашаа', 123371),\n",
       " ('zakharsmirnoff', 95867),\n",
       " ('каникулах', 142954),\n",
       " ('займёмся', 133987),\n",
       " ('пабликом', 177982),\n",
       " ('vorontsovlife', 90078),\n",
       " ('жанне', 130768),\n",
       " ('привет', 192955),\n",
       " ('олимпийский', 173060),\n",
       " ('военный', 113582),\n",
       " ('городок', 121030),\n",
       " ('мэтро', 161274),\n",
       " ('вы', 116178),\n",
       " ('центре', 235460),\n",
       " ('ага', 98073),\n",
       " ('angie_enzorry', 12772),\n",
       " ('lnovopashina', 52692),\n",
       " ('никому', 168002),\n",
       " ('нужна', 169242),\n",
       " ('никто', 168014),\n",
       " ('верит', 111018),\n",
       " ('channelone_rus', 20527),\n",
       " ('gavr_yelena', 33526),\n",
       " ('ленин', 152087),\n",
       " ('любимый', 154258),\n",
       " ('франциск', 231441),\n",
       " ('_mrs____horan', 7726),\n",
       " ('пока', 186654),\n",
       " ('думаю', 128715),\n",
       " ('открытку', 175849),\n",
       " ('время', 115140),\n",
       " ('veronichka201', 88259),\n",
       " ('верон', 111105),\n",
       " ('мы', 161027),\n",
       " ('обе', 169826),\n",
       " ('знаем', 138086),\n",
       " ('сама', 205800),\n",
       " ('себе', 207651),\n",
       " ('испортила', 141307),\n",
       " ('типичная', 221571),\n",
       " ('vikapincher', 88820),\n",
       " ('по', 183372),\n",
       " ('моему', 159265),\n",
       " ('только', 222098),\n",
       " ('одна', 172165),\n",
       " ('девочка', 123827),\n",
       " ('волошина', 114120),\n",
       " ('она', 173238),\n",
       " ('хорошо', 233906),\n",
       " ('принимала', 194011),\n",
       " ('спросили', 215031),\n",
       " ('выучила', 118052),\n",
       " ('serjcold', 77040),\n",
       " ('планшет', 182748),\n",
       " ('упорно', 227691),\n",
       " ('говорит', 120307),\n",
       " ('новых', 168559),\n",
       " ('обновлений', 170577),\n",
       " ('sonnymoore1989', 79390),\n",
       " ('para', 67714),\n",
       " ('noir', 63847),\n",
       " ('мэнсона', 161246),\n",
       " ('плеере', 182961),\n",
       " ('включился', 112796),\n",
       " ('проста', 196922),\n",
       " ('такое', 219336),\n",
       " ('сильно', 208979),\n",
       " ('скучаю', 210329),\n",
       " ('умереть', 227073),\n",
       " ('зашел', 137088),\n",
       " ('паблик', 177974),\n",
       " ('версус', 111146),\n",
       " ('баттл', 103785),\n",
       " ('очень', 177695),\n",
       " ('рад', 199237),\n",
       " ('видеть', 112071),\n",
       " ('репера', 203020),\n",
       " ('atl', 15276),\n",
       " ('из', 139133),\n",
       " ('новочебоксарска', 168535),\n",
       " ('aztecs', 15897),\n",
       " ('rubin_paceka', 74457),\n",
       " ('kononenkome', 47331),\n",
       " ('он', 173236),\n",
       " ('ещё', 130573),\n",
       " ('сам', 205799),\n",
       " ('во', 113404),\n",
       " ('множественном', 159029),\n",
       " ('числе', 236958),\n",
       " ('обращается', 170978),\n",
       " ('доброе', 126365),\n",
       " ('утро', 228806),\n",
       " ('сволочи', 207198),\n",
       " ('идти', 139083),\n",
       " ('спать', 214360),\n",
       " ('тут', 224283),\n",
       " ('позвонил', 186254),\n",
       " ('друг', 128399),\n",
       " ('где', 119076),\n",
       " ('минут', 158480),\n",
       " ('15', 957),\n",
       " ('компанией', 146722),\n",
       " ('заедет', 133712),\n",
       " ('kipigulu', 46440),\n",
       " ('тема', 220739),\n",
       " ('нашей', 165035),\n",
       " ('исторички', 141556),\n",
       " ('геи', 119107),\n",
       " ('лесбиянки', 152308),\n",
       " ('гомофобы', 120742),\n",
       " ('ogoreltseva1', 65236),\n",
       " ('нее', 165793),\n",
       " ('ковырялся', 145864),\n",
       " ('смотри', 211651),\n",
       " ('почесал', 191116),\n",
       " ('будний', 108461),\n",
       " ('день', 124444),\n",
       " ('пойти', 186651),\n",
       " ('трэвэлэрс', 223807),\n",
       " ('коффи', 148537),\n",
       " ('бесплатный', 105063),\n",
       " ('завтрак', 132945),\n",
       " ('chlenschmidta', 20911),\n",
       " ('нуу', 169361),\n",
       " ('была', 109157),\n",
       " ('контрольная', 147286),\n",
       " ('надеюсь', 162161),\n",
       " ('нормально', 168834),\n",
       " ('сделал', 207516),\n",
       " ('слушала', 211121),\n",
       " ('альбомы', 99596),\n",
       " ('канье', 143022),\n",
       " ('тоже', 221878),\n",
       " ('нескольких', 167208),\n",
       " ('песнях', 181322),\n",
       " ('слышала', 211151),\n",
       " ('упоминания', 227665),\n",
       " ('майкле', 155058),\n",
       " ('лидия', 152631),\n",
       " ('раевская', 199404),\n",
       " ('взрослые', 111879),\n",
       " ('игрушки', 138939),\n",
       " ('сборник', 206575),\n",
       " ('креативов', 149052),\n",
       " ('бегаю', 104022),\n",
       " ('ржу', 203503),\n",
       " ('ниипет', 167882),\n",
       " ('кстати', 149985),\n",
       " ('чуть', 237670),\n",
       " ('ли', 152551),\n",
       " ('единственная', 129921),\n",
       " ('книга', 145681),\n",
       " ('от', 175013),\n",
       " ('какой', 142525),\n",
       " ('напиток', 163410),\n",
       " ('чаще', 236073),\n",
       " ('всего', 115284),\n",
       " ('пьешь', 198760),\n",
       " ('утрам', 228764),\n",
       " ('пью', 198762),\n",
       " ('vmblf9psda', 89791),\n",
       " ('alishakotova', 11289),\n",
       " ('аааа', 97336),\n",
       " ('конченные', 147464),\n",
       " ('бляяяя', 106415),\n",
       " ('хочу', 234105),\n",
       " ('ааа', 97335),\n",
       " ('новый', 168547),\n",
       " ('год', 120353),\n",
       " ('подвинься', 184451),\n",
       " ('пару', 178882),\n",
       " ('дней', 126126),\n",
       " ('ничего', 168219),\n",
       " ('успеваю', 228261),\n",
       " ('noimann_', 63846),\n",
       " ('мою', 160252),\n",
       " ('запись', 135415),\n",
       " ('петушков', 181444),\n",
       " ('до', 126199),\n",
       " ('сих', 209400),\n",
       " ('пор', 189079),\n",
       " ('админы', 98321),\n",
       " ('удалили', 225783),\n",
       " ('dddddddd', 24248),\n",
       " ('dynamo_ru', 27396),\n",
       " ('ееее', 129985),\n",
       " ('первые', 179691),\n",
       " ('чемпионы', 236333),\n",
       " ('figurymytyv', 31404),\n",
       " ('пора', 189080),\n",
       " ('мириться', 158567),\n",
       " ('завтра', 132937),\n",
       " ('зарплаты', 135995),\n",
       " ('tz4tjhf2vc', 85457),\n",
       " ('коммов', 146657),\n",
       " ('беру', 104953),\n",
       " ('гооу', 120836),\n",
       " ('фото', 231212),\n",
       " ('nxcytmvjty', 64533),\n",
       " ('рисуют', 203733),\n",
       " ('понимаю', 188410),\n",
       " ('рисовать', 203701),\n",
       " ('этот', 241371),\n",
       " ('череп', 236402),\n",
       " ('тупо', 224095),\n",
       " ('сил', 208921),\n",
       " ('sleepwalker_121', 78583),\n",
       " ('аз', 98439),\n",
       " ('мудак', 160388),\n",
       " ('буду', 108479),\n",
       " ('злить', 137960),\n",
       " ('ладно', 151204),\n",
       " ('беда', 104069),\n",
       " ('подруга', 185384),\n",
       " ('бросила', 108171),\n",
       " ('перед', 179962),\n",
       " ('самым', 206081),\n",
       " ('новым', 168557),\n",
       " ('годом', 120403),\n",
       " ('оборвав', 170773),\n",
       " ('тем', 220738),\n",
       " ('праздники', 191683),\n",
       " ('yporoti_nosorog', 94939),\n",
       " ('миссия', 158642),\n",
       " ('выполнена', 117341),\n",
       " ('забыл', 132603),\n",
       " ('погуляли', 184235),\n",
       " ('благодарна', 105820),\n",
       " ('такие', 219306),\n",
       " ('моменты', 159646),\n",
       " ('seryacoval', 77070),\n",
       " ('вендис', 110911),\n",
       " ('жутко', 132094),\n",
       " ('жирная', 131668),\n",
       " ('еда', 129851),\n",
       " ('против', 197135),\n",
       " ('хоть', 234005),\n",
       " ('мало', 155407),\n",
       " ('мальски', 155520),\n",
       " ('натурального', 164583),\n",
       " ('фаст', 229923),\n",
       " ('фуда', 231672),\n",
       " ('snatovichmarina', 78921),\n",
       " ('месяц', 157643),\n",
       " ('января', 242398),\n",
       " ('kyazymovaeminka', 49385),\n",
       " ('kamabambucha', 44166),\n",
       " ('видела', 112010),\n",
       " ('хаха', 232580),\n",
       " ('больше', 107175),\n",
       " ('смотреть', 211646),\n",
       " ('хахаха', 232596),\n",
       " ('cunningmadman', 22307),\n",
       " ('музыка', 160577),\n",
       " ('моём', 160270),\n",
       " ('плейлисте', 182974),\n",
       " ('делится', 124263),\n",
       " ('умирать', 227142),\n",
       " ('убивать', 225072),\n",
       " ('двигаться', 123500),\n",
       " ('майкл', 155056),\n",
       " ('джексон', 125086),\n",
       " ('вдохно', 110533),\n",
       " ('ivanova231297', 41145),\n",
       " ('еле', 130140),\n",
       " ('встала', 115615),\n",
       " ('никуда', 168022),\n",
       " ('alchemistlily', 9884),\n",
       " ('сессия', 208585),\n",
       " ('махнемся', 156667),\n",
       " ('удачи', 225869),\n",
       " ('lt', 53362),\n",
       " ('черт', 236530),\n",
       " ('него', 165510),\n",
       " ('снова', 212039),\n",
       " ('захотелось', 136906),\n",
       " ('тату', 219902),\n",
       " ('последние', 189729),\n",
       " ('попрощаюсь', 188963),\n",
       " ('россией', 204527),\n",
       " ('июля', 141848),\n",
       " ('jonny_30', 42706),\n",
       " ('конец', 146992),\n",
       " ('света', 206892),\n",
       " ('голливудски', 120471),\n",
       " ('очередной', 177719),\n",
       " ('самостеб', 205989),\n",
       " ('стесняются', 216232),\n",
       " ('люди', 154440),\n",
       " ('стебаться', 216054),\n",
       " ('над', 162065),\n",
       " ('собой', 212297),\n",
       " ('убить', 225147),\n",
       " ('эму', 240872),\n",
       " ('уотсон', 227553),\n",
       " ('максик', 155255),\n",
       " ('анфоловил', 100562),\n",
       " ('скпздц', 210052),\n",
       " ('дальше', 123061),\n",
       " ('жить', 131731),\n",
       " ('отучусь', 177003),\n",
       " ('10', 529),\n",
       " ('классов', 145286),\n",
       " ('чите', 237090),\n",
       " ('потом', 190542),\n",
       " ('бабушке', 102707),\n",
       " ('дедушке', 124010),\n",
       " ('бишкек', 105773),\n",
       " ('vano_0', 87685),\n",
       " ('але', 99232),\n",
       " ('це', 235304),\n",
       " ('вже', 111618),\n",
       " ('прикольно', 193714),\n",
       " ('5ий', 4377),\n",
       " ('кашляти', 144024),\n",
       " ('слоним', 210957),\n",
       " ('прослушивание', 196797),\n",
       " ('30', 2615),\n",
       " ('выезжать', 116690),\n",
       " ('думала', 128707),\n",
       " ('высплюсь', 117806),\n",
       " ('надо', 162184),\n",
       " ('набраться', 161672),\n",
       " ('моральных', 159827),\n",
       " ('физических', 230345),\n",
       " ('голосовых', 120605),\n",
       " ('мож', 159271),\n",
       " ('паранормальные', 178628),\n",
       " ('явления', 242081),\n",
       " ('забрать', 132528),\n",
       " ('сном', 212057),\n",
       " ('посмотреть', 189862),\n",
       " ('срочно', 215335),\n",
       " ('одеяло', 172079),\n",
       " ('офис', 177282),\n",
       " ('укутаться', 226716),\n",
       " ('вся', 115778),\n",
       " ('пиздец', 181849),\n",
       " ('придумала', 193320),\n",
       " ('классный', 145281),\n",
       " ('твитт', 220192),\n",
       " ('решила', 203421),\n",
       " ('написать', 163400),\n",
       " ('забыла', 132604),\n",
       " ('этом', 241354),\n",
       " ('лах', 151683),\n",
       " ('сами', 205836),\n",
       " ('шахтеры', 238183),\n",
       " ('vxqiek7qvr', 90564),\n",
       " ('австралийский', 97824),\n",
       " ('доллар', 127166),\n",
       " ('канадского', 142877),\n",
       " ('попрет', 188897),\n",
       " ('вниз', 113311),\n",
       " ('выставляем', 117824),\n",
       " ('сделки', 207550),\n",
       " ('продажу', 195537),\n",
       " ('отличный', 175975),\n",
       " ('m6gtowi6km', 54086),\n",
       " ('дефицит', 124902),\n",
       " ('секретного', 207872),\n",
       " ('ингредиента', 140185),\n",
       " ('новосибирске', 168511),\n",
       " ('janenyforest', 41738),\n",
       " ('женька', 131227),\n",
       " ('спасибо', 214265),\n",
       " ('shinya_oppa', 77608),\n",
       " ('trickyguzzler', 84654),\n",
       " ('reno_oppa', 72988),\n",
       " ('свэг', 207238),\n",
       " ('бэйби', 109294),\n",
       " ('5srq5gr7pu', 4279),\n",
       " ('несколько', 167209),\n",
       " ('рождения', 204118),\n",
       " ('могу', 159144),\n",
       " ('поверить', 183675),\n",
       " ('uporoty_cloud', 86566),\n",
       " ('зная', 138242),\n",
       " ('почту', 191232),\n",
       " ('россии', 204528),\n",
       " ('лет', 152364),\n",
       " ('сорок', 213507),\n",
       " ('успокоишься', 228339),\n",
       " ('nok_3250', 63868),\n",
       " ('умный', 227200),\n",
       " ('стал', 215608),\n",
       " ('делаем', 124193),\n",
       " ('лабораторную', 151083),\n",
       " ('физике', 230323),\n",
       " ('ищем', 141821),\n",
       " ('страницу', 216720),\n",
       " ('написана', 163384),\n",
       " ('alina_komarek', 10939),\n",
       " ('кузнецов', 150160),\n",
       " ('бедный', 104106),\n",
       " ('сообщ', 213273),\n",
       " ('сми', 211524),\n",
       " ('нам', 163101),\n",
       " ('остаёться', 174865),\n",
       " ('гордиться', 120901),\n",
       " ('идиотка', 139048),\n",
       " ('сначала', 211794),\n",
       " ('скажу', 209450),\n",
       " ('жалею', 130689),\n",
       " ('своих', 207182),\n",
       " ('словах', 210828),\n",
       " ('разревелась', 200535),\n",
       " ('увидела', 225362),\n",
       " ('старушку', 215900),\n",
       " ('около', 172741),\n",
       " ('универмага', 227348),\n",
       " ('работе', 199122),\n",
       " ('украсить', 226626),\n",
       " ('всё', 115804),\n",
       " ('7c0dcrbicn', 5086),\n",
       " ('поняла', 188495),\n",
       " ('включала', 112763),\n",
       " ('свой', 207183),\n",
       " ('комп', 146713),\n",
       " ('дня', 126189),\n",
       " ('свою', 207218),\n",
       " ('священную', 207326),\n",
       " ('музыку', 160610),\n",
       " ('chapaeff', 20537),\n",
       " ('berillii', 17338),\n",
       " ('тобой', 221794),\n",
       " ('спорю', 214884),\n",
       " ('кроме', 149532),\n",
       " ('говорю', 120320),\n",
       " ('ввп', 110467),\n",
       " ('гавеный', 118450),\n",
       " ('показатель', 186689),\n",
       " ('wylsacom', 92283),\n",
       " ('валь', 109895),\n",
       " ('помоги', 188140),\n",
       " ('внеси', 113287),\n",
       " ('мой', 159360),\n",
       " ('udid', 85779),\n",
       " ('обзор', 170015),\n",
       " ('сделать', 207542),\n",
       " ('юлички', 241688),\n",
       " ('получится', 187773),\n",
       " ('открытом', 175855),\n",
       " ('уроке', 228004),\n",
       " ('самая', 205824),\n",
       " ('клевая', 145327),\n",
       " ('училка', 229222),\n",
       " ('ссы', 215456),\n",
       " ('пятилетняя', 198895),\n",
       " ('помни', 188107),\n",
       " ('можешь', 159289),\n",
       " ('бессильна', 105171),\n",
       " ('своей', 207169),\n",
       " ('ленью', 152197),\n",
       " ('теперь', 220913),\n",
       " ('начну', 165011),\n",
       " ('делать', 124219),\n",
       " ('дела', 124186),\n",
       " ('повторение', 183902),\n",
       " ('той', 221906),\n",
       " ('жаркой', 130834),\n",
       " ('летней', 152415),\n",
       " ('ночи', 169046),\n",
       " ('shaposhnikov_av', 77363),\n",
       " ('fedorov_selsky', 31104),\n",
       " ('георгий', 119355),\n",
       " ('ждем', 130932),\n",
       " ('дебаты', 123714),\n",
       " ('16', 1018),\n",
       " ('го', 120193),\n",
       " ('kotigaswim', 47701),\n",
       " ('устал', 228387),\n",
       " ('ыы', 240031),\n",
       " ('сендвич', 208164),\n",
       " ('бонусным', 107353),\n",
       " ('помидором', 188067),\n",
       " ('составе', 213680),\n",
       " ('robinson_corn', 73754),\n",
       " ('моим', 159350),\n",
       " ('единственным', 129932),\n",
       " ('любимым', 154262),\n",
       " ('печали', 181472),\n",
       " ('qidvcipgep', 71341),\n",
       " ('владушка', 113002),\n",
       " ('оладушка', 172921),\n",
       " ('сказал', 209463),\n",
       " ('обидеться', 170061),\n",
       " ('останусь', 174840),\n",
       " ('стипендии', 216327),\n",
       " ('окрашивание', 172805),\n",
       " ('придется', 193283),\n",
       " ('просить', 196759),\n",
       " ('родителей', 203977),\n",
       " ('папу', 178538),\n",
       " ('инфаркт', 140724),\n",
       " ('хватит', 232772),\n",
       " ('узнает', 226386),\n",
       " ('сколько', 209895),\n",
       " ('крашусь', 149018),\n",
       " ('белые', 104742),\n",
       " ('мухи', 160912),\n",
       " ('халатиках', 232164),\n",
       " ('космооднокласник', 148163),\n",
       " ('хахах', 232595),\n",
       " ('w_dancer', 90829),\n",
       " ('рядом', 205413),\n",
       " ('такими', 219319),\n",
       " ('стыдится', 217289),\n",
       " ('признаки', 193527),\n",
       " ('несовместимости', 167259),\n",
       " ('тросика', 223616),\n",
       " ('блокировки', 106213),\n",
       " ('вилки', 112317),\n",
       " ('устанавливается', 228409),\n",
       " ('переднее', 180045),\n",
       " ('крыло', 149861),\n",
       " ('sks', 78435),\n",
       " ('shockblade', 77706),\n",
       " ('заб', 132261),\n",
       " ('collvolthunmo', 21576),\n",
       " ('бодрого', 106618),\n",
       " ('утречка', 228797),\n",
       " ('декабря', 124142),\n",
       " ('вторник', 115872),\n",
       " ('прав', 191513),\n",
       " ('человека', 236224),\n",
       " ('исполняя', 141276),\n",
       " ('обязанности', 171602),\n",
       " ('забывайте', 132594),\n",
       " ('правах', 191518),\n",
       " ('походу', 190940),\n",
       " ('оставляют', 174742),\n",
       " ('второй', 115882),\n",
       " ('xddd', 92640),\n",
       " ('единственный', 129931),\n",
       " ('семье', 208135),\n",
       " ('подобный', 185048),\n",
       " ('инцидент', 140801),\n",
       " ('dubrasha', 27051),\n",
       " ('new_upgrate', 62737),\n",
       " ('слеследующем', 210700),\n",
       " ('ребят', 202368),\n",
       " ('yaltashaurma777', 93748),\n",
       " ('интересно', 140560),\n",
       " ('ей', 130084),\n",
       " ('грудь', 121945),\n",
       " ('показывает', 186708),\n",
       " ('чтоли', 237350),\n",
       " ('kindr_96', 46404),\n",
       " ('мама', 155578),\n",
       " ('бешенстве', 105321),\n",
       " ('едет', 129864),\n",
       " ('москвы', 160062),\n",
       " ('них', 168173),\n",
       " ('проблемы', 194968),\n",
       " ('транспорт', 222901),\n",
       " ('спбу', 214375),\n",
       " ('ру', 204717),\n",
       " ('приказал', 193613),\n",
       " ('долго', 127078),\n",
       " ('n_nastya97', 60631),\n",
       " ('jon_invincible', 42690),\n",
       " ('аналогично', 99947),\n",
       " ('спортсмен', 214870),\n",
       " ('выдающийся', 116530),\n",
       " ('господи', 121201),\n",
       " ('бы', 109109),\n",
       " ('выздоровел', 116765),\n",
       " ('___okeah___', 6643),\n",
       " ('значит', 138211),\n",
       " ('предстоит', 192282),\n",
       " ('devo4ka_shihova', 24972),\n",
       " ('страны', 216765),\n",
       " ('уехать', 226184),\n",
       " ('они', 173271),\n",
       " ('пристали', 194429),\n",
       " ('блиин', 106101),\n",
       " ('обламываюсь', 170238),\n",
       " ('мои', 159345),\n",
       " ('поиски', 186577),\n",
       " ('привели', 192948),\n",
       " ('результату', 202749),\n",
       " ('эта', 241265),\n",
       " ('мысль', 161089),\n",
       " ('дает', 122922),\n",
       " ('покоя', 186937),\n",
       " ('плохо', 183194),\n",
       " ('спалось', 214155),\n",
       " ('этого', 241340),\n",
       " ('lenoreignore', 50698),\n",
       " ('их', 141733),\n",
       " ('неделю', 165609),\n",
       " ('прослушала', 196790),\n",
       " ('заскроблилось', 136152),\n",
       " ('уэн', 229421),\n",
       " ('роум', 204630),\n",
       " ('гуинд', 122353),\n",
       " ('уи', 226422),\n",
       " ('зэ', 138613),\n",
       " ('лайонс', 151338),\n",
       " ('фри', 231530),\n",
       " ('оф', 177218),\n",
       " ('кализиамс', 142625),\n",
       " ('tellebi', 82890),\n",
       " ('дададададааааааааа', 122906),\n",
       " ('норм', 168812),\n",
       " ('болтал', 107049),\n",
       " ('длинным', 126051),\n",
       " ('волосам', 114089),\n",
       " ('тайлера', 219227),\n",
       " ('жирный', 131682),\n",
       " ('пудель', 198092),\n",
       " ('роберт', 203836),\n",
       " ('0axtq', 207),\n",
       " ('кого', 145894),\n",
       " ('хотелось', 233978),\n",
       " ('поскорее', 189683),\n",
       " ('встретить', 115664),\n",
       " ('папуууу', 178555),\n",
       " ('t8jhsckdlq', 81840),\n",
       " ('therealguf', 83398),\n",
       " ('бляха', 106404),\n",
       " ('муха', 160899),\n",
       " ('хитяру', 233236),\n",
       " ('обломали', 170337),\n",
       " ('obnuli', 64959),\n",
       " ('будущий', 108500),\n",
       " ('лошади', 153776),\n",
       " ('поэтому', 191391),\n",
       " ('ржем', 203494),\n",
       " ('пашем', 179280),\n",
       " ('крч', 149848),\n",
       " ('звонит', 137399),\n",
       " ('брат', 107783),\n",
       " ('просит', 196756),\n",
       " ('выйти', 116857),\n",
       " ('выхожу', 118126),\n",
       " ('открываю', 175823),\n",
       " ('дверь', 123477),\n",
       " ('налетаю', 163023),\n",
       " ('зелема', 137680),\n",
       " ('шишка', 238759),\n",
       " ('лбу', 151714),\n",
       " ('vikaskvor', 88836),\n",
       " ('найдут', 162626),\n",
       " ('noutka', 64050),\n",
       " ('даааа', 122705),\n",
       " ('идея', 139022),\n",
       " ('после', 189715),\n",
       " ('дашиной', 123386),\n",
       " ('рекламы', 202861),\n",
       " ('боюсь', 107660),\n",
       " ('лета', 152365),\n",
       " ('зубы', 138530),\n",
       " ('могут', 159145),\n",
       " ('дожить', 126796),\n",
       " ('стеной', 216139),\n",
       " ('веселье', 111302),\n",
       " ('пьяные', 198797),\n",
       " ('мужские', 160498),\n",
       " ('голоса', 120582),\n",
       " ('поют', 191399),\n",
       " ('руссконародном', 205072),\n",
       " ('духе', 128942),\n",
       " ('смешно', 211483),\n",
       " ('looooozer69', 52929),\n",
       " ('готовлюсь', 121327),\n",
       " ('сессии', 208576),\n",
       " ('началось', 164881),\n",
       " ('пообщалась', 188570),\n",
       " ('полицейским', 187348),\n",
       " ('справедливость', 214941),\n",
       " ('соседи', 213563),\n",
       " ('нами', 163194),\n",
       " ('делают', 124227),\n",
       " ('ремонт', 202962),\n",
       " ('месяца', 157644),\n",
       " ('кривые', 149254),\n",
       " ('ноги', 168575),\n",
       " ('оч', 177641),\n",
       " ('начинаю', 164970),\n",
       " ('активную', 99052),\n",
       " ('деятельность', 124957),\n",
       " ('зиму', 137907),\n",
       " ('носом', 168932),\n",
       " ('разблокируют', 199529),\n",
       " ('телефоны', 220669),\n",
       " ('оглядываются', 171757),\n",
       " ('типа', 221549),\n",
       " ('палева', 178219),\n",
       " ('вижу', 112158),\n",
       " ('подслушано_зима', 185507),\n",
       " ('наконец', 162849),\n",
       " ('посмотрела', 189858),\n",
       " ('worldsendmovie', 91944),\n",
       " ('восторге', 114687),\n",
       " ('отличное', 175965),\n",
       " ('завершение', 132765),\n",
       " ('sonyakazhan', 79437),\n",
       " ('kopteva1709', 47404),\n",
       " ('умеем', 227030),\n",
       " ('чем', 236296),\n",
       " ('налаживаются', 163005),\n",
       " ('отношения', 176219),\n",
       " ('irishka_ryazan', 40631),\n",
       " ('америку', 99781),\n",
       " ('начала', 164867),\n",
       " ('lentaruofficial', 50708),\n",
       " ('navalny', 62218),\n",
       " ('ломоносов', 153579),\n",
       " ('говорил', 120300),\n",
       " ('flutiepie', 31832),\n",
       " ('бля', 106342),\n",
       " ('отправляются', 176467),\n",
       " ('трудом', 223734),\n",
       " ('приходят', 194642),\n",
       " ('твои', 220288),\n",
       " ('__kotrpa__', 6731),\n",
       " ('гитаре', 119706),\n",
       " ('училась', 229216),\n",
       " ('года', 120354),\n",
       " ('тяжело', 224705),\n",
       " ('самой', 205899),\n",
       " ('учиться', 229274),\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vec.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.76     27957\n",
      "    positive       0.77      0.76      0.77     28752\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a56F0vivTZL"
   },
   "source": [
    "Попробуем сделать то же самое для триграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3))\n",
    "bow = vec.fit_transform(x_train) \n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "6WhD1jOrvTZL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.47      0.57     27957\n",
      "    positive       0.61      0.82      0.70     28752\n",
      "\n",
      "    accuracy                           0.65     56709\n",
      "   macro avg       0.67      0.65      0.64     56709\n",
      "weighted avg       0.67      0.65      0.64     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred_thrgramm = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred_thrgramm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае стало хуже :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyW9lA_6vTZL"
   },
   "source": [
    "А теперь для TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "vec_train = vec.fit_transform(x_train)\n",
    "vec_test = vec.transform(x_test)\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "vec_train = scaler.fit_transform(vec_train)\n",
    "vec_test = scaler.transform(vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "B4jqKUFgvTZL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.75      0.76     27957\n",
      "    positive       0.76      0.78      0.77     28752\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec_train, y_train)\n",
    "pred_tfidf = clf.predict(vec_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixapx51FvTZM"
   },
   "source": [
    "## О важности эксплоративного анализа\n",
    "\n",
    "Но иногда пунктуация бывает и не шумом — главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
    "bow = vec.fit_transform(x_train) \n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "BSxY-8aRvTZM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.97      0.96     27957\n",
      "    positive       0.97      0.95      0.96     28752\n",
      "\n",
      "    accuracy                           0.96     56709\n",
      "   macro avg       0.96      0.96      0.96     56709\n",
      "weighted avg       0.96      0.96      0.96     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_8uz4TevTZM"
   },
   "source": [
    "Стоило оставить пунктуацию — и внезапно все метрики устремились к 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите признаки с самыми большими коэффициентами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "bRbaKt3qvTZM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327972"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZC8Mb6bvTZM"
   },
   "source": [
    "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "vvWdQbYMvTZN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92     27957\n",
      "    positive       1.00      0.83      0.91     28752\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.93      0.92      0.91     56709\n",
      "weighted avg       0.93      0.91      0.91     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "3nD71B-ZvTZO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“@Fashionbar_uz: Ladies Monday . Каждый понедельник для всех девушек кальяны от заведения. #fashionbar http://t.co/ZlvxOcZPR4” zaviduyu))\n",
      "Гримерка-лук))) #evabristol #performance #gig #vocaldiva #гастроли #гитис #театр http://t.co/S60OvyyTS6\n",
      "RT @alivfedorov: http://t.co/DvYLJaPHxR Девушки это самые хитрые создания! так что даже не пытайся их обмануть!:)\n",
      "@pavelsheremet @ukrpravda_news @varlamov хотя исторически правильно желто-синий:)\n",
      "Понятия не имею чем меня привлекла эта картинка!)))http://t.co/twqP8zyh1A\n",
      "@Sveta12126 ну или пусть Лиама пришлет ко мне) своего младшенького. Как сказала Кэтрин: \"Это нормально - любить двоих\" ахах\n",
      "Вышел в свет новый каталог запасных частей и деталей ТМК! Звоните - подарим :)\n",
      "@u_alekseeva_17 видужуй:3\n",
      "Я там тепер буду кожну середу)\n",
      "29-й выпуск Дроидкаста будет не против ваших плюсов на Хабре   ;)\n",
      "@nemoniga а я вот знаю :) нам на политической географии рассказывал душечка Гурин\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "tweets_with_cool_token = [tweet for tweet in x_train if cool_token in tweet]\n",
    "np.random.seed(42)\n",
    "for tweet in np.random.choice(tweets_with_cool_token, size=10, replace=False):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv6P8ncIvTZO"
   },
   "source": [
    "## Символьные n-граммы\n",
    "\n",
    "Теперь в качестве признаков используем, например, униграммы символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), analyzer='char')\n",
    "bow = vec.fit_transform(x_train) \n",
    "bow_test = vec.transform(x_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "cATsmweivTZO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.97      0.98     27957\n",
      "    positive       0.98      0.99      0.98     28752\n",
      "\n",
      "    accuracy                           0.98     56709\n",
      "   macro avg       0.98      0.98      0.98     56709\n",
      "weighted avg       0.98      0.98      0.98     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PK8-_xwvTZO"
   },
   "source": [
    "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или иначе, на символах классифицировать тоже можно: для некоторых задач (например, для определения языка) признаки-символьные n-граммы могут внести серьезный вклад в качество модели.\n",
    "\n",
    "Ещё одна замечательная особенность признаков-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoXhAroLvTZP"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "- https://books.google.com/ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "islq37BzvTZP"
   },
   "source": [
    "## Регулярные выражения\n",
    "\n",
    "https://ru.wikipedia.org/wiki/Регулярные_выражения\n",
    "\n",
    "Вообще, часто бывает так, что для конкретного случая нужен особый способ токенизации, и надо самостоятельно написать регулярное выражение. Или, например, перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
    "\n",
    "Навык полезный, давайте в нём тоже потренируемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "ymGN9r6gvTZP"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qJxZd2pvTZQ"
   },
   "source": [
    "### findall\n",
    "возвращает список всех найденных совпадений\n",
    "\n",
    "- ? : ноль или одно повторение\n",
    "- \\* : ноль или более повторений\n",
    "- \\+ : одно или более повторений\n",
    "- . : любой символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Hn6AnrvtvTZQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abca']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHOTT9XTvTZQ"
   },
   "source": [
    "Вопрос на внимательность: почему нет abcx?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "k7gzrPCOvTZR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbbca']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('ab+c.', 'abbbca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qxi1znlvTZR"
   },
   "source": [
    "### split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "z3Qz-3RSvTZR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie', ' weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ur6WK3GvTZR"
   },
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Pwu8L0LevTZS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie, weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGFk3MpOvTZS"
   },
   "source": [
    "### sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "azeAmGOBvTZS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcbbc\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('a', 'b', 'abcabc')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом в качестве repl, можно передавать не только строку, но и функцию, которая принимает на вход [Match](https://docs.python.org/3/library/re.html#match-objects) объект. Можно делать что-то типо этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(a#1)bc(a#2)bc'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def count(match):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    return f'(a#{counter})'\n",
    "\n",
    "re.sub('a', count, 'abcabc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, c объектами типа re.Match работают и многие другие методы re. Например, метод re.finditer в отличии от re.findall будет возвращать те самые re.Match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='abcd'>\n",
      "<re.Match object; span=(11, 15), match='abca'>\n"
     ]
    }
   ],
   "source": [
    "for match in re.finditer('ab+c.', 'abcdefghijkabcabcxabc'):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо найденных строчек объекты Match также, например, содержат информацию о позиции найденного \"совпадения\" в строке (span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY8kwgOVvTZS"
   },
   "source": [
    "### compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Nudrx7nJvTZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "\n",
    "prog = re.compile('[А-Яа-яё\\-]+')\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONOWD3XvTZS"
   },
   "source": [
    "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
    "\n",
    "```\n",
    "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "JLWpguIsvTZT"
   },
   "outputs": [],
   "source": [
    "emails = 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "ayTzy2lCvTZT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = re.compile('@[a-z\\.]+')\n",
    "prog.findall(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoHLF2IQvTZT"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://habr.com/ru/post/115825/\n",
    "- https://www.regular-expressions.info/\n",
    "- https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## И еще немножко про категориальные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже говорили, что кодировать категориальные признаки просто в виде чисел  — не очень хорошая идея.\n",
    "Это задаёт некоторый порядок, которого на категориальных переменных может и не быть. \n",
    "\n",
    "Существует два основных способа обработки категориальных значений:\n",
    "- One-hot-кодирование\n",
    "- Счётчики (CTR, mean-target кодирование, ...) — каждый категориальный признак заменяется на среднее значение целевой переменной по всем объектам, имеющим одинаковое значение в этом признаке.\n",
    "\n",
    "Основная идея счетчиков заключается в том, что нам важны не сами категории, а значения целевой переменной, которые имеют объекты этой категории. Каждый категориальный признак мы заменим средним значением целевой переменной по всем объектам этой же категории. Формально это можно записать так:\n",
    "$$\n",
    "g_j(x, X) = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)][y_i = +1]}{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним методы на нашем любимом Титанике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/iad34/seminars/master/materials/data_sem1.csv\",\n",
    "                   sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этот раз не будем сильно страдать с обработкой пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = data['Age'].fillna(data.groupby('Pclass')['Age'].transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Cabin',axis=1,inplace=True)\n",
    "data.drop('Name',axis=1,inplace=True)\n",
    "data.drop('Ticket',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889, 9)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "y = data_ohe['Survived']\n",
    "data_ohe = data_ohe.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_unknown</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass   Age  SibSp  Parch     Fare  Sex_male  Sex_unknown  \\\n",
       "0            1       3  22.0      1      0   7.2500         1            0   \n",
       "1            2       1  38.0      1      0  71.2833         0            0   \n",
       "2            3       3  26.0      0      0   7.9250         0            0   \n",
       "3            4       1  35.0      1      0  53.1000         0            0   \n",
       "4            5       3  35.0      0      0   8.0500         1            0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7702528379772962\n"
     ]
    }
   ],
   "source": [
    "X_train ,X_test, y_train, y_test = train_test_split(data_ohe,\n",
    "                                                    y, test_size=0.2, \n",
    "                                                    random_state=42, shuffle=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(roc_auc_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ,X_test, y_train, y_test = train_test_split(data,\n",
    "                                                    y, test_size=0.2, \n",
    "                                                    random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target = X_train.groupby('Sex')['Survived'].mean() \n",
    "X_train.loc[:, 'Sex'] = X_train['Sex'].replace(mean_target) \n",
    "X_test.loc[:, 'Sex'] = X_test['Sex'].replace(mean_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target_e = X_train.groupby('Embarked')['Survived'].mean() \n",
    "X_train.loc[:, 'Embarked'] = X_train['Embarked'].replace(mean_target_e) \n",
    "X_test.loc[:, 'Embarked'] = X_test['Embarked'].replace(mean_target_e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['Survived'], axis=1, inplace=True) \n",
    "X_test.drop(['Survived'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7927494802494804\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(roc_auc_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодирование признаков с помощью счетчиков приведённом выше может приводить к переобучению. Почему?\n",
    "\n",
    "Чтобы бороться с этим, можно экспериментировать с разными модификациями:\n",
    "\n",
    "1. Вычислять значение счётчика по всем объектам расположенным выше в датасете (например, если у нас выборка отсортирована по времени).\n",
    "2. Вычислять по фолдам, то есть делить выборку на некоторое количество частей и подсчитывать значение признаков по всем фолдам кроме текущего (как делается в кросс-валидации).\n",
    "3. Вносить некоторый шума в посчитанные признаки. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of sem10_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
